## Designing Data-Intensive Applications: The Big Ideas Behind Reliable, Scalable, and Maintainable Systems

by Martin Kleppmann

### Part 1: Foundations of Data Systems

#### Ch 1: Reliable, Scalable, and Maintainable Applications
- Some trends for today's data systems:
     - New tools for data storage and processing blurs the boundaries of traditional databases, e.g. Redis, Kafka.
     - Each application is architectured as a conglomerate of components, each with a specific purpose, e.g. db + cache + search index + message queue.
- Reliability
     - Meaning the system continues to work correctly even in the presence of faults (fault tolerant, resilient).
     - Hardware faults: disk failures are super common in large data centers. Solution is usually data redundency.
     - Software errors: leap seconds, slow/unresponsive dependent/upstream service, cascading failures. No magic solution, but can do testing/monitoring/self checking assumptions at runtime.
     - Human errors: usually misconfig. Solutions include designing the system carefully, using sandboxes, adopting good operational procedure/tools (e.g. quick rollback), monitoring and telemetry.
     - Practical tip: to exercise the system for reliability we could do chaos engineering -- introduce faults manually.
- Scalability
     - The system's ability to deal with increased load, with some definition of "load" -- differ by individual cases.
     - Performance is usually discribed by a distribution.
          - In SLAs performance usually use percentiles: the 50, 90, 99, 99.9th percentile of response times.
          - Long queues of requests often slow down performance significantly -- head-of-line blocking.
          - The tails are important because a page's performance is dominated by the slowest responding concurrent request.
     - We can scale up and scale out (stateless apps usually easier to scale out), but there's no magic sauce -- the scalable architecture depends on the volume of read and writes, access patterns, data complexity and performance requirements.
- Maintainability
     - Operability: provide observability to internal states/metrics, minimize surprises in behavior, design with portability.
     - Simplicity: use proper abstraction to avoid accidental complexity.
     - Evolvability: anticipate future changes, and be extensible to architecture-level changes.

Chapter 2 Data Models and Query Languages
• ideas
- data abstraction layers: the application layer (api, objects), db data structures layer (as JSON documents, or tables etc), db implementation layer (memory, disk, network), physical layer (circuits)
- relational model: transaction processing, batch processing
- relational model vs document model
     - object-relational mismatch, solution: ORM
     - or document model (JSON = explicit tree structure of an object), better locality (no need JOIN queries)
     - document model: flexible schema, better locality, maybe closer to data structures of application
     - relational model: better joins, many-to-one and many-to-many relationships
-  document based db is good for one-to-many relationships, but bad for many-to-many relationships: have limited support for joins and that put the burden of joins onto application code
     - possible to denormalize, but application code need to keep denormalized data consistent
     - emulate joins usually slower than specialized code built into the db
- relational db's query optimizer takes care of the execution
- better to keep documents small: a read loads the entire document, and a write usually rewrites the entire document
- DB query languages
     - SQL is declarative as well as CSS
     - declarative languages hide implementation (imperative languages), so easier to optimize and write
     - mongodb has some way of supporting mapreduce; also some declaratives for aggregations
- graph-like models
     - good for: flexible schema, complicated interrelated data
     - vertex: id, outgoing edges, incoming edges, properties (key-value)
     - edge: id, tail vertex, head vertex, relationship label, properties (key-value)
     - Cypher: delaring vertices and edges
     - SPARQL(triple store): (subject, predicate, object) -- predicate indicates edge or property
     - Datalog: rule matches -- each rule generates something like a "new record in the db"
• todos
- think about how different real world situations things/apps and what they do, and reverse engineer about their data model
- figure out how mongoengine makes queries (at low level) and read code about db multi reference
• vocab
polyglot persistence: use different data stores in the same application

Chapter 3 Storage and Retrieval
- indexes: additional data structure; speed up reads but slows down writes
- hash indexes
     - requirement: append-only db (log), key-value data, not many keys (index fits into memory)
     - stores: dictionary of (key, byte offset)
     - good for: lots of writes and not many distinct keys
     - bad for: range queries for keys
     - multiple segments of records (logs): offline(background thread) compaction and segment merging (deletes obsolete records and deleted records) -- write to new segment, then delete old segments
     - segments usually several kb and size varies by segment
     - read: search for latest segment (each segment has a hash), then 2nd to latest etc, keep number of segments small
     - crash recovery: for hash, store shots of hash on disk; for records, have checksums
     - synchronization: one write head, concurrent reads (because  records immutable)
- SSTable (Sorted String Table) (also LSM-tree)
     - requirement: key-value data
     - good for: range queries, dataset much bigger than available memory, high write throughput (because disk writes are sequential)
     - on disk, store SSTables(key-value pairs, sorted by key); in memory, store sparse index for each SSTable to speed up lookup
     - write: write to memtable (in-memory balanced data structure), when memtable's size > threshold, write to disk as a SSTable
     - read: look up for key in memtable first, then latest SSTable (using index to know which chunk it's supposed to be in, then read chunk sequentially), second to latest SSTable...
     - in background, merge and compaction: multi-way sorted merge, take the value of the latest SSTable
     - crash recovery: memtable is lost, so also store unsorted sequential log while memtable is not written to disk and recover from that history
     - LSM-tree (log-structured merge tree) optimizations: use bloom filters to decide if a key doesn't exist; strategies to merge SSTables
     - example: Lucene uses LSM-tree to store (term, list of docs that contains the term)
- B-Tree
     - on disk data structure
     - fixed size(4KB) pages -- when rewrite, overwrite the entire page
     - branching factor usually a couple of hundreds (500?), so 4 levels usually enough
     - read: follow the pointers
     - write: modify in place -- follow pointers and rewrite leaf pages; if leaf's parent is full, break up leaf's parent into 2 pages; delete is tricky
     - crash recovery: write-ahead-log(WAL) before acting on B-Tree to prevent data corruption
     - concurrency: lock on every page
     - optimizations
          - copy-on-write: when update a page, copy the page then update it's address in parent
          - B+ tree: in lower level pages, store only part of the key to increase branch factor
          - leaf pages and sibling pages to aid large range reads (avoid another trip to parent)
- LSM-Tree vs B-Tree
item
LSM-Tree
B-Tree
disk space
smaller, more compact
larger, more segmented
write throughput
typically higher because of lower write amplification
higher, because have to write multiple pages sometimes and write aplification (write-ahead-logs), and have to rewrite the entire page for a couple of bytes
read throughput
1. has to look into multiple data structures and SSTables (worsen by compaction not keeping up), 2. high percentile response times might be large because of waiting for a compaction to finish
predictable
when write requests are high
1. compaction cannot keep up, so data unmerged, so read becomes slow 2. initial write (logging and flushing memtable to disk) has to share write resources with compaction
-
transactions semantics (locks)
-
each key only in 1 single place, so locks can be implemented easily
- other indexing structures
     - clustered index: storing values (with keys also); covering index: store keys and some columns but not all columns
     - multi-column indexes: concatenated index (concat 2 columns, but can't look up 2nd+ col); multi-dimensional index with multi-d range queries: translate 2d to a single number using a space filling curve -- good for geospatial data and others
     - fuzzy index: levenshtien automaton for searching edit distance
     - in-memory database: RAM cheaper, can be distributed, needs write log+snapshot to disk to persist, advantageous because they data don't need to encode in-memory data structures into a disk form -- lots of data structures easier/only available in memory, anti-caching for data larger than memory
- Transaction Processing and Analytics
     - have different db access patterns, so db usually separate
     - OLTP bottleneck is seek time; OLAP bottleneck is disk bandwidth
     - for analytics, it's called data warehousing., usually uses star/snowflakes schema and SQL interface. fact table usually wide (hundreds of columns)
- column oriented storage
     - popular within OLAP (analysis): better than row oriented because in analysis only few cols will be read
     - for finite num of values greatly less than number of record, can use column compression -- bit map then run-length encoding
     - optimizations: use memory bandwidth between main memory and CPU cache, and vectorized processing in CPU
     - sort: sort by 1 col, then by 2nd col etc, helps with data filtering and better compression. can also have several different sort orders
     - write is slow -- writing using LSM tree -- first go in memory, then write to disk in bulk
     - data cubes, materialized views: precomputed data in multiple dimensions
• todos
- look up what's a Bloom Filter and how it's used
- look up how B-Tree works: insert, look up, delete (how it balances itself)

Chapter 4 Encoding and Evolution
- backward compatibility: new code reads old data. forward compatibility: old code read new data
- language specific formats usually is language bound, inefficient, prone to attacks (instantiate objects), and don't have versioning support
- JSON doesn't distinguish integers with floating points, nor does it support binary strings (sequences of bytes without readable text) -- but it supports Unicode
- binary encoding saves space, and have versioning
     - e.g. Thrift and Protocol Buffers defines schema and with tags(numbers) to fields, so reference data with field tag. also variable length so there are length indicators as well as datatype indicators
     - compatibility is limited to some fields (required/nonrequired etc), field names, and some lists
     - Apache Avro
          - Avro has specified schema, and binary data only encode data itself
          - schema evolution done by auto resolving reader's schema with writer's schema: order of fields, new fields (set to default val), removed fields (ignored and must have default value)
          - got writer's schema from beginning of file (lots of records) or record version (have a db for schemas of all versions)
          - good for auto generated schema (don't haven't to hand assign field number etc)
          - good for dynamically typed languages -- no need to do type check
- data flow through DBs
     - in rolling upgrades, both old and new code runs, so data need to consider both forward and backward compatibility, especially if old code drop an added field made by new code
     - db snapshots: better write these records in the same schema
- data flow through services: REST and RPC
     - REST wins: allows client and server to be deployed and evolved independently, evolution made by adding optional request params/adding new fields
     - RPCs: try to make a remote request look like a local request. RPC libraries nowadays have futures/promises to encapsulate asynchronous actions that might fail
- data flow through message passing
     - advantages: buffer for slowness, auto redelivery--reliability, avoids sender knowing IP of recipient, multiple recipients for same msg, sender just sends (logical decoupling)
     - the msg queue usually have flexible data model, but also should consider backward/forward compatibility

#### Chapter 5: Replication
- benefits for data replication
     - low latency (different geographical regions) (roundtrip CA -> Netherlands: 150ms)
     - better scaling (more read load)
     - high availability/tolerance
- master-slave/leader-follower
     - 1 leader to handle writes; leader tells followers to write by sending replication logs; followers are read-only
     - synchronous vs asynchronous replication on write
          - never have all synchronous because that breaks HA
          - semi-synchronous: one sync replica + other async replicas -- so that we always have 2 up-to-date copies of data
          - async: good: continue to service writes even all followers fall behind; bad: weak durability(if leader fails unreplicated changes are lost)
     - setting up new followers
          periodically store consistent snapshots of the leader; copy the snapshot to the new follower, then apply logs later than that
     - failover
          - if follower fails, just catch up with the leader by asking for the write logs later than what it already has and replaying them
          - if leader fails,
               1. detect failure via timeout
               2. select new leader (concensus)
               3. reconfigure all nodes (including the old leader if/when it comes back) to recognize new leader
          - possible issues
               - new leader has a lag from old leader (if async replication), so data might be lost, or has conflicts when old leader comes back up (especially bad if other storages like cache depends on a lagging new leader)
               - split-brain: 2 leaders at the same time, resulting in data conflict/corruption
               - too short of a timeout results in unneeded failovers and further delays system, if system already at high load or with network glitches
     - implementing replication logs
          - pass queries from leader to follower: potential issue being some queries underterministic (RAND), dependent on current db (UPDATE WHERE), or side effects (user defined functions)
          - pass WAL (write ahead logs) to follower: readily available, but issue being WAL usually low level (bytes on disk blocks), making it hard for replicas to run to different software versions, requiring downtime when upgrade
          - pass logical (row-based) logs to follower: logs recording which rows are changed, decoupling with storage details; also good for exporting to external applications
          - trigger based (application level): in relational dbs, a trigger can be fired to log a change in a table when a write transaction commits, so other applications can use it later. more error prone than built-in replication
- problems with replication lag (1 sec+)
     - possible causes
          - machine near capacity
          - network issues
          - recovering from failure
     - problem #1: read-your-own-write consistency
          - requires a read after write should show instantly
          - solutions
               - if something could be modified, read from leader by routing the request to leader's datacenter (doesn't work if everything's modifiable)
               - for 1 min after last write, read from leader
               - client remembers (logical or actual) timestamp of last write, server finds a replica that lags below this for the next read
               - cross-device: member of last write needs to be centralized; route to same datacenter for all devices
     - problem #2: monotonic reads
          - problem: read from first a more up-to-date replica, then a lagging replica; observes data moving back in time
          - solution: ensure each user always read from the same replica (e.g. route with hash of userId) unless the replica fails
     - problem #3: consistent prefix reads
          - problem: when 2 causally related data lands in different partitions, they replicate at different rates (partitions operate independently, no global ordering of events for partitioned systems); may see logically reversed contents
          - solution: application makes sure causually related content are put in the same partition (may not be efficient); or use the tracking algorithm to track causal dependencies
     - bottom line
          - application code needs to be aware of replication lag
          - db could provide distributed transactions (partitioned and replicated) as a guarantee, but at a cost of performance and availability
- multi-leader replication
     - write to any leader, and leaders have ways to sync up asynchronously themselves
     - benefits
          - write performance: otherwise all writes has to go cross-DC onto that single leader
          - tolerance of datacenter outages (one leader per DC)
          - tolerance of network problems: public network not as reliable; single leader is sensitive to network since all writes are synchronous
     -  use cases/scenarios for multi-leader
          - multiple data centers: 1 leader per DC; between DCs leaders replicate to each other after conflict resolution
          - applications/clients that still works without internet connection, e.g. in a calendar app, viewing/changing meetings while offline means it's 1 leader/DC per device, and network extremely unreliable
          - collaborative editing: one user/device as a db leader for faster collaboration, or single-leader transaction for slower collaboration (locking)
     - write conflicts
          - happens when two writes goes into 2 different leaders and both gets OK -- need to resolve asynchronously
          - usually applies to individual row/records, even in a multi-write transaction
          - solution #1: avoiding it: ensure each record is handled by only 1 datacenter; breaks when rerouting traffic to another datacenter etc
          - solution #2: resolve in a convergent way
               - take the write with higher write ID/replica ID/timestamp(aka Last Write Wins); loses data
               - merge values (concat for strings by alphabetical order)
               - record the conflict in a data structure, and write application code to resolve conflict
          - application code for conflict resolution
               - on write: predefine a handler code and run it instantly at conflict detection
               - on read: when read, run code (either instantly or prompt user to resolve), then write back
     - communication topologies among multiple leaders
          - circular, star/tree
               - data passes through multiple nodes, so need to remember which replica the data has passed through in the rep log to ignore it when arrives at self
               - problem: one single failed node interrupts the flow, reconfiguration is manual/tricky
          - all-to-all
               - problem: when some leaders' network is faster than others, a leader receives time-reversed replication requests
               - cannot use timestamp (clock skew)
               - can use version vectors to order these events
- leaderless replication (e.g. Amazon Dynamo)
     - write to all n replicas at the same time and wait for OK from w replicas, read from all n replicas and wait for r values, then select the newest value by versioning
     - quorum consistency: when w + r > n, read and write overlaps -- guaranteed to have the newest value; usually pick odd n with w = r = (n + 1) / 2
     - previously down replica to catch up
          - read repair: when read, client sees stale value so client writes back to that replica; good for frequently read records
          - anti-entropy process: background process to check and repair inconsistencies; without this values rarely read may have reduced durability
     - multi datacenter
          - option 1: send write to all nodes in all dcs, read usually wait for the local nodes because they're fast
          - option 2: send read and write locally, and cross-dc replication happens async like multi-leader model
     - limitations for quorum consistency even when w + r > n
          - if using sloppy quorum, then reads are no longer guaranteed to overlap with writes (because writes have temporarily moved to other nodes)
               - sloppy quorum: when number of nodes >> n (maybe the db is partitioned) and network partitition happens, the db still accepts writes from clients cut off from the usual n nodes, and store data temporarily on a different set of nodes
               - hinted handoff: write the value back to the original nodes when they're back
          - concurrent writes cause conflicts -- has to merge values, since timestamped LWW suffers from clock skew
          - for concurrent write and read, values returned to read is undetermined
          - if write acks < w, values are not rolled back on succeeded nodes -- later reads may see the value from the failed write
          - number of replicas holding new value falls below w, if we copy data from a stale replica when a replica fails
     - monitoring staleness
          - for leader based dbs, just measure the difference between the follower's position in replication log from the leader's
          - for leaderless dbs, not common in practice
     - concurrent writes: detection and resolution
          - problem: events arrive in different order on different nodes, due to network delay and paritial failures; but nodes should converge to a consistent value regardless to be eventually consistent
          - Last Write Wins (LWW)
               - force an ordering of writes, e.g. timestamp, discard earlier ones
               - cost for durability: may even drop nonconcurrent writes (timestamps are not reliable)
               - the only safe way to use LWW is when a key gets written only once (immutable records): e.g. assign a UUID per write as the key
          - concurrency is determined by whether two events know of each other, regardless of their actual timing
               - A causal relationship is not concurrent, e.g. insert a=3 and update a=5 are not concurrent, but insert a=3 and insert a=5 are
          - making concurrent writes
               - example
               ![shopping cart example](images/ddia-5-13.png)
               - for each key, server maintains multiple <version #, value> pairs with the latest version number -- it increments the latest version number with every write, and stores the version number together with the value written
               - client must read before a write to make sure it's seeing the most up to date value from the server
               - on read request, server returns all values (not overwritten) with the latest version number
               - when writing, client merges all values received in previous read (either using custom implementation or data structures like CRDT), add its change, and include the version number that the change and merge is based on (use tombstone for deletes)
               - on receiving a write request with version number n, server overwrites all values with version number n or below (they're already merged by client), but keep values with versions higher than n (they're concurrent with this write)
               - for multiple replicas, use one version # per replica per key, forming a version vector
- questions
     - how do you take a consistent snapshot while serving reads and writes?
     - conflict resolution: 2-way merge(CRDT) vs 3-way merge(mergeable persistent data structures), what's the difference?

#### Chapter 6: Partitioning
- each node like an independent db, only that they collectively return the right answer; complex queries may need multiple partitions to cooperate
- one node may host multiple partitions and replicas, and is a leader in some partitions and follower in others
- ways to partition
     - goal: spread data/query load evenly, avoid hot spots
     - by key range
          - keep key in sorted order and set boundaries between keys (chosen manually or automatically)
          - good: range queries efficient
          - bad: possible hot spots on some queries -- can use an alternative key
          - rebalancing: dynamic (split to 2 when partition size over a threshold)
     - by hash of the key
          - hash keys using a good hash function (e.g. md5); boundary evenly spaced, or chosen randomly (a.k.a. consistent hashing)
          - good: less prone to hot spots
          - bad: no range queries
          - rebalancing: common to have a fixed number of partitions then move the entire partition; can also use dynamic partitioning
     - compound
          - compound primary key: first column by hash (for partitioning), and later columns concatenated for sorting
          - good: supports range queries with a fixed partition key to model one-to-many relationships, e.g. (userId, timestamp)
- skewed workloads
     - skewed workloads happen for situations like celebrity
     - solution without db native support: adding a 2 digit random number to spread the load to 100 nodes
          - but reads now requires to hit all 100 nodes
          - and this needs additional bookkeeping to know which key are split and how they're split
- partitioning with secondary indexes
     - secondary indexes: usually one key (secondary index) corresponds to multiple rows for search; doesn't map to partitions easily since partitions are done with the primary key
     - document-partitioned indexes (local indexes)
          - each partition keeps its own index
          - good: fast writes because it only writes to one partition
          - bad: scatter-gather; need to read from all partitions so it's expensive and prone to tail latency amplification
     - term-partitioned indexes (global indexes) 
          - global index of all data, stored in partitions based on term (range scans), or hash of term (even distribution)
          - good: read is fast -- only query the index then go to the actual data
          - bad: write needs to be on multiple partitions because index of different terms are on different nodes, requiring distributed transaction support (in practice index updates are usually async, so reading index immediately after write doesn't give the updated data)
- rebalancing
     - moving load from one node to another
     - scenarios: query throughput increases, need more CPUs to handle; data size increases, need more disk/RAM to store; machine failure, need replacement
     - requirements: fair share of loads, continue to serve read+write while rebalancing, move only minimal amount of data
     - fixed number of partitions
          - make lots of partitions (p >> n), only move entire partitions across nodes
          - partition size proportional to data size
          - challenging to decide on the number of partitions (partitions incur maintenance overhead, but when partitions are large, rebalancing/recovery is expensive), especially when data size is highly variable
     - dynamic partitioning
          - split and merge partitions as the size of the partitions change
          - number of partitions grows proportionally to data size
          - good for adaption to data size; with an empty db, you can configure initial partitioning
          - can be used for both key-range and hashed partitions
     - partitioning proportional to nodes
          - fixed number of partitions per node, since num of nodes usually grow with data size, partition size is usually stable
          - when a new node joins, it randomly splits a fixed number of existing partitions and move those splits into the new node (consistent hashing)
          - partitions are split by random, so requires hashed partitioning (could create unfair splits, but mitigated with p >> n)
     - operations: rebalancing is expensive (moving huge chunks of data and rerouting), and auto-detection + auto-rebalancing can exacerbate a problem -- need some human intervention
- request routing
     - knowledge of which partition is kept at which node can be kept in one of these places
          - all nodes: request hits a random node (IP not changing as much, so enough to use DNS), then forwarded to the owning node
          - a routing tier: forwards requests to the owning node; needs to have HA too
          - the client(s): directly connects to the owning node
     - knowledge is distributed across different places, so usually delegates the knowledge to Zookeeper, and have nodes/client subscribe to Zookeeper updates
          - zookeeper stores (key-range, partition, node, IP address)
     - alternatively in the all-nodes scenario, can use a gossip protocol so it doesn't rely on external service discovery
     - in data warehouses, MPP(massively parallel processing) query optimizer breaks down a complex query into execution stages on partitions for parallel execution

#### Chapter 7 Transactions
- why
     - provides an abstraction to simplify the programming model by grouping several reads+writes into a unit
     - either commits or aborts, so applications don't have to worry about partial failure, and can safely retry
- the safety guarantees of transactions: ACID (meaning is ambiguous -- mostly a marketing term)
     - atomicity (abortability): guarantee that if a group of several writes fails halfway, the group can be retried safely, because the db has undone any writes so far
     - consistency: application data is in good state (invariants still hold) -- but it's determined by application, not the db
     - isolation
          - concurrent transactions are isolated from each other: one transaction (a transaction may include a few writes on different objects) can't see another's uncommitted writes
          - e.g. of violation to isolation: the concurrent read-modify-write
          - also called serializability -- concurrent as if actions are made in serial
     - durability
          - once transaction is committed, it will not be forgotten even in case of hardware failure or crash
          - in a standalone db it means data is written to disk; in a replicated db it means data has been written to several nodes
          - but nothing's absolute; disks can fail, ssd can get corrupted, power/node outage for in-memory dbs etc
- single object and multi object transactions
     - always provided: dbs universally provide atomicity and isolation on the level of a single object via log (for crash recovery) and lock (for isolation) on objects
     - sometimes provided: to prevent data loss in a concurrent scenarios, some dbs provide automic increment, and/or CAS (compare-and-set) -- allowing the set to succeed only if value is not changed by someone else (otherwise retry)
     - not all provides: transactions on a group of multiple objects is tricky to implement on a distributed scenario, and is costly on performance, but there's still need for multi object transactions
          - in relational model, foreign key reference requires updating multiple tables
          - in denormalized document db, different fields and documents need to be updated (e.g. email records has a is_unread field, and mailbox records has a num_unreads field to avoid counting unreads)
          - for secondary indexes, both the index and the doc needs to be updated
     - handling errors and aborts
          - best effort dbs (like leaderless) requires better handling in application level
          - problem with retries
               - if action succeeded but network back failed, a retry replays the action, unless there's application dedup in there
               - retry due to overload worsens the problem, should limit the number of retries, use potential backoff, and handle overload issues separately (is it even possible?)
               - retry is only useful to transient errors (deadlock, isolation violation, network, failover etc)
               - side effects might be duped because they happen even if abort; e.g. sending the email twice, so should use two phase commit (2PC)
               - if the data in client is kept only in memory, and during the (lengthy since there are backoff) retry process if the client failed, data is lost
- It's not easy to test for concurrency issues in a large database (hard to reproduce and not knowing which other code is accessing the db), but serializable isolation has a performance cost, so dbs try to provide some weaker level of isolation.
- weak isolation levels
     - read committed
          - only read committed data (no dirty reads), and only overwrite committed data (no dirty writes)
          - no dirty reads prevents problems: other readers won't see partial updates if updates on multiple objects, and if aborts then uncommitted data won't be read by others
          - no dirty writes prevents problems: when 2 transactions both act on 2 same objects, and a transaction enters early and exits late, it's 1st value written is lost to the 2nd transaction
          ![dirty write example](images/ddia-7-5.png)
          - does NOT guard against: read-modify-write data races (e.g. 2 concurrent counter increments), since the 1st write is already committed; or the repeatable reads problem
          - implementation
               - dirty writes prevented by row-level locks (don't release lock until transaction completes)
               - dirty reads prevented by the same locks (unpopular because of performance dip from long running transactions), or keeping both an uncommitted version and an old version and serve the old version while the write transaction is running
     - snapshot isolation/repeatable read
          - nonrepeatable read/read skew: a read transaction reads 2 objects, and in between the 2 reads, a committed write transaction modifies both obj (e.g. account transfer) -- but this read result won't be repeated
          ![nonrepeatable read example](images/ddia-7-6.png)
          - when nonrepeatable read can't be tolerated (long running, read-only transactions)
               - db backup may take hours, and it can't read nonrepeatable result, otherwise it'll backup inconsistent values
               - analysis or integrity checks touches a large part of db, and it might fail if seeing these transiently inconsistent data
          - snapshot isolation prevents it: each read transaction sees a consistent snapshot of the db at a particular point in time (at the start of itself); even if data is changed by some later transaction, they won't be seen
          - implementation
               ![MVCC example](images/ddia-7-7.png)
               - generalizing on keeping 2 versions (old and during transaction), the db keeps several different committed version of db -- MVCC (multi-version concurrency control)
               - each transaction assigned an always-increasing transactionId
               - each db row stores multiple versions, each version has fields: id, value(s), createdBy (transactionId), deletedBy
               - each write operate translates to a delete and a create, and modifies the row metadata accordingly
               - reads observes an object(i.e. row metadata entry) value only if both the 2 conditions are true (requires compiling a list of all in-progress transactionIds when the transaction starts):
                    - at the time that this read transaction started, the transaction creating this object is already committed
                    - the object is either not marked for deletion, or it is but the transactionId isn't committed at the time this read transaction starts
          - in effect, readers never block writers, and writers never block readers
          - ways indexes work for a multi-version db
               - can have index pointing to all versions of the object, and filter by visibility given the transactionId, then garbage collects the index entries together with old versions
               - use append-only, copy-on-write B-tree (similar to [git](https://github.blog/2020-12-17-commits-are-snapshots-not-diffs/)) for the entire db -- each transaction creates a new tree root; also needs garbage collection
     - preventing lost updates
          - examples of read-modify-write: account balance read then update, adding an item to a list of JSON, concurrently editing wiki articles (save the entire document)
          - atomic write operations
               - usually implemented with locking on the object before read and release after write
               - alternatively can force all atomic writes to be executed by 1 single thread
          - explicit locking
               - e.g. for multi-player control of an avatar, automic write doesn't apply since we also need to integrate logic to check move isValid within the locking section
               - prone to errors because people forget to lock
          - auto detect lost updates
               - if detected, force one writer to abort and retry
               - can be implemented efficiently with snapshot isolation
               - better than explicit locking because it doesn't require users to do anything
          - compare-and-set
               - when updating, compare with old value and set only when data hasn't been changed
          - when there's replication, concurrent writes happen on multiple nodes
               - since there's no single up-to-date copy of the data, lock and compare-and-set wouldn't be effective (because they're non-deterministic?)
               - atomic operations still works, best for commutative operations (increments, adding to a set etc)
               - best to hold multiple versions of values (siblings) and resolve later on
     - write skew and phantoms
          - examples of write skew
               - both doctors select table of (name, is_on_call) to see if num_oncall > 2 at the same time, see yes under snapshot isolation, and both commits to take their name off call
                    - solution: explicitly lock all rows (WHERE oncall = true) to disallow further reads and writes until the update is complete
               - two organizers check if the same room is taken during the same meeting period by checking, see no under snapshot isolation, and both book the slot
                    - solution: materializing conflicts. Create a table of for every room every 15 min for the next 6 months, lock these slots upon querying and release after the update
               - 2 players controlling the same character tries to move it to the same coordinate, checks if there isn't already another item on the coordinate, see yes under snapshot isolation, and both move to the same position
               - two users try to create the same username by checking if this name is taken at the same time, see ok under snapshot isolation, and both take it
                    - solution: use the db's unique constraint (2nd attempt will force an abort)
               - a user spend game money concurrently by first checking if the remaining balance would be > 0, see yes under snapshot isolation, and both commits the action
          - how does the problem happen
               1. a select query checks if some condition is satisfied by searching for multiple rows that satisfies some condition
               2. depending on the result, the application code either aborts or goes ahead
               3. going ahead, the application code modifies one or multiple objects so that it changes the precondition returned in step 1
          - write skew is the generalization of lost updates (read-modify-write) -- two transactions concurrently read a few objects and updates some of them. When they update the same object, it's a dirty write or a lost update depending on the timing
          - phantom: the effect that a write in one transaction changes the result of a search query in another transaction
          - solutions
               - similar to detecting lost updates, use a db that provides true serializable isolation level
               - use db constraints (e.g. unique)
               - lock up all obj during the first read query (including materializing conflicts)
          - materializing conflicts
               - if query checks for the absence of something, have a table that materialize it so that they could be locked up; this turns the phantom into concrete rows to lock on
               - a last resort because it's error prone and ugly
- serializability
     - strongest isolation level, guarantees that all transactions have result same as serial
     - why: other weaker isolation levels are hard to understand (implementation wise), it's hard to debug a race, and there's no good tools to detect a race
     - actual serial execution
          - execute on one thread on one core
          - on a recent development because
               - RAM is now cheaper so we can store all data in memory, so each transaction is fast enough
               - OLTP is usually shorter than analysis, and analysis is usually read only so it's served on a snapshot
          - transactions needs to be fast to unblock other ones: encapsulate transactions as stored procedures (each procedure includes multiple db accesses) so it avoids waiting for multiple IO and locking overhead
               - modern stored procedures uses existing general purpose programming languages
          - caveats of single thread execution
               - limited to data that fits into memory; if data not in memory then it should abort the transaction, load data into memory asynchronously, then retry
               - write throughput needs to be low enough to be handled by single thread
               - partitioning may speed it up (one thread per partition), but if coordination between partitions is required, it's very slow (e.g. secondary indexing) since it requires actual lockstep execution across multiple partitions
     - Two-phase locking (2PL)
          - reads can be concurrent, but once there's a write, read is blocked by write and write is blocked by read until transaction is committed/aborted
          - implementation
               - locks in shared-mode (readers) and exclusive-mode (writers)
               - once acquires a lock, must hold until commit or abort
               - deadlock happens frequently because of so many locks involved -- db detects deadlocks automatically, and aborts one of them to later retry by application
          - performance
               - overhead to acquire and release locks, especially since so many locks are involved, aborts and retries are common
               - reduced concurrency: transactions can be any length, and even one long transaction stalls subsequent transactions
               - deadlock can happen more frequently
               - overall, 2PL dbs have unstable throughput with long tails
          - predicate locks and index-range locks
               - predicate lock: a lock in the forms of a predicate (a SELECT clause) that belongs to all objects (even future nonexistent objects) -- also with shared and exclusive mode
               - predicate lock + 2PL prevents all forms of write skew and race
               - index-range(a.k.a. next-key) locks lock by index range as approximation of predicate lock in implementation; e.g. meeting room bookings has indicies either by time range or by room, so it locks on the targeted time range of all rooms, or this room at all times
               - index-range locks lock a superset of the required objects on the index, so it usually achieves better performance than predicate locks
               - if index not available, the db locks on the entire table
     - Serializable Snapshot Isolation (SSI)
          - a fairly new idea that's been implemented by both single node and distributed db
          - optimistic concurrency control: based on snapshot isolation, don't block until commit time to detect serialization conflicts
          - idea: make sure to detect if read is stale -- abort if it's the case
               - case 1: detecting read of stale MVCC objects
                    ![detecting read of stale MVCC objects example](images/ddia-7-10.png)
                    - event sequence: A reads, A writes, B reads (reads a stale MVCC obj), B writes, A commits, B aborts
                    - transaction monitor detects B reading uncommitted change made by A
                    - wait for commit time to decide whether OK or abort, because A might have aborted or B could be just read-only
               - case 2: detecting write that affects previous read
                    ![detecting write that affects previous read example](images/ddia-7-11.png)
                    - event sequence: A reads, B reads, A writes (notify B), B writes (notify A), A commits, B aborts
                    - use index-range lock (or similar mechanisms) to detect and notify each other
                    - db only needs to remember what data has been read until all concurrent transactions are done
                    - the first committer wins, others abort
          - performance
               - better than pessimistic when load is not too high; otherwise causes too many retries that worsen the load
               - also depends on implementation details like tracking granularity that affects overhead
               - can expand to replicas and partitions, achieving high throughput
               - response time less variable -- reads run on consistent snapshots without locks so it's good for read-heavy applications, but long writes more likely to meet a conflict and abort

#### Chapter 8 The Trouble with Distributed Systems
- unreliable networks
     - can't distinguish between: bad/slow outgoing network, unavailable recipient, busy receipient (load spike or doing GC), or bad/slow incoming network
     - detecting faults
          - timeout is the most general way to detect faults
          - sometimes there are explicit messages: if process crashes the os may send RST/FIN TCP packet or even notify other nodes, may be access network switches at physical level etc
     - timeout
          - if too low, might be false positive, overloading the entire system which is already overloaded, or premature failover leading to an action performed twice
          - unlike telephone lines that establish a preset channel end-to-end for each session (bounded delay), TCP is designed to handle bursty traffic, resulting in unbounded delays
          - can choose timeout by experiments and/or auto-adjust with jitter
     - network congestion and queuing
          - causes
               - if multiple senders send packets to the same desination, a network switch has to queue them up and feed them one by one into the destination link, or dropping packets if the queue is full
               - a destination machine with busy CPU cores queues incoming requests from network (by the OS) until the application is ready to handle it
               - an VM monitor buffers/queues incoming data from network when another VM tenant uses the CPU core
               - TCP performs flow control so queueing may happen at the sender (even before data enters the network)
               - TCP auto-transmits packets that timeout, resulting in increase in observed delay
           - in public clouds and multi-tenant datacenters, network links/switches are shared (or even network interface+CPUs are shared in VMs), noisy neighbor can consume lots of resources (e.g. MapReduce batch workloads)
- unreliable clocks
     - time-of-day clocks
          - synchronized via NTP (Network Time Protocol), may jump back
          - not suitable to measure elapsed time since it may jump back
     - monotonic clocks
          - always move forward, but absolute value meaning different things on different servers (even CPUs on the same server)
          - NTP may adjust how fast it moves forward
          - suitable to measure elapsed time to the microseconds
     - possible issues with clock synchronization
          - servers have clock drifts up to 17 seconds/day if synchronized once per day, so the clock may jump back or leap forward
          - NTP firewalled/misconfiguration makes servers unable to synchronize for a long time
          - synchronization accuracy itself is limited by internet delay, and in the case of congestion the client may abort synchronization 
          - NTP servers may or may not smear the leap second, since some systems are not aware of the leap second
          - VM sees virtualized clock, and the hypervisor context switch results in VMs seeing clocks jumping forward
          - if app run on a device you don't control (someone else's phone) the time may be intentionally wrong
     - relying on synchronized clock
          - if an application relies on synchronized clocks, it should monitor clock drifts closely because it causes subtle failures
          - e.g. timestamps for ordering events
               - multi leader db propagates writes to follower, by "last write wins" principle, the write on faster clock wins, causing dropped writes from the lagging clock
               - LWW dbs can't distinguish between writes happen in quick succession, or truly concurrent ones -- has to use version vectors to track causality
               - may have two requests with the same timestamp (on different machines), so LWW dbs need a tiebreaker rule
               - NTP doesn't solve the problem because NTP itself has internet delay so it's accuracy is limited
               - can use logical clocks instead of time-of-day clock or monotonic clock
          - clock reading have a confidence interval, but no way to estimate the interval
               - for snapshot isolation in a distributed db, generating a monotonically increasing transaction ids becomes a bottleneck (for a single instance db a counter is sufficient)
               - Google's Spanner implementation waits for the confidence interval before commit read-write transactions to make sure the read transactions don't overlap with the read-write transaction
     - process pauses
          - example: get incoming request -> check if lease expiry is at least 10 seconds later otherwise renew lease -> if lease is valid (if thread is preempted here for over 10 seconds, it'll process the request with an expired lease), process request (solution: use a fencing token on the server side)
          - possible causes
               - garbage collection stops the world (can spend up to minutes)
               - on virtual machines, can get suspended and restarted for live migration
               - on laptops, can get suspended when user closes down the laptop
               - operating system context switch
               - unexpected wait for disk IO (if disk is via network then even more delay)
               - os's frequent swapping to disk (thrashing)
               - getting SIGSTOP and SIGCONT signals (e.g. CTRL-Z)
          - there are real-time response guarantees for life-critical embedded systems, but it's expensive and limited
          - tricks to reduce GC impacts: treat GC as planned maintenance time and have other nodes take over during GC, or use GC for short-lived objects that are fast to collect then periodically restart nodes like rolling upgrades
- knowledge, truth and lies
     - truth is defined by majority
          - problem: a node gets stopped by GC when lease expires, meanwhile another node gets lease and writes data. after first node comes back it tries to write -- then data corrupted
          - solution: fencing. The distributed locking service distributes an increasing fencing id to each lock holder, and server rejects updates from older fencing id.
          ![server checks the fencing token before accepting requests](images/ddia-8-5.png)
     - byzantine faults
          - faults when some nodes are malicious or working deliberately incorrectly
          - used in aerospace environments (radiation breaks memory) or multi-party (bitcoin), but too expensive in other places
          - because all/most nodes runs the same software, byzantine fault tolerant algos won't save the situation
          - can put sanity checks: checksums in application level protocol, sanitize inputs, multiple NTP addresses
     - system model and reality (theoretical vs implementation)
          - timing models
               - synchronous model: bounded network delay, clock error, process pauses -- unrealistic
               - partially synchronous model: bounded most of the time but occasionally can be bad -- realistic
               - asynchronous model: no timing assumptions (can't use clock at all) -- restrictive
          - system models
               - crash-stop faults: when faults happen, node crashes and never comes back
               - crash-recovery faults: after crash, takes some unknown time to come back -- disk gets persisted, and memory doesn't
               - byzantine faults: nodes to anything including deceiving
          - evaluating distributed algos
               - correctness -- listing properties of an algo and make sure it behaves this way
               - safety and liveliness
                    - safety: something always hold, and no bad things happen because once a damage is done there's no going back
                    - liveliness: something eventually happens (e.g. eventual consistency)
               - models are simplifications of reality (implementation) but nevertheless provides great insights

Chapter 9 Consistency and Concensus
- Linearizability
     - the illusion that there's only 1 copy of data, and all operations on it is atomic -- once an operation is executed, all subsequent operations conforms with it -- no going back
     - is a recency guarantee -- concurrent operations may be either old or new value, but subsequent ones are all new vals
     - examples that relies on linearizability
          - distributed locking and leader selection: the lock needs to be on  linearizable datastroe
          - uniqueness guarantee/constraints in distributed db: like a "lock" on the unique value
          - multi channel timing dependencies: if more than 1 channel to communicate, e.g. message queue should happen after db store, if db is not linearizable, the msg queue processing that depends on data being there might find the data not there
     - implementation
          - single leader -- might be linearizable
               - only if read from master too, but it's not when failover (could even lose data), or having split brain
          - multi leader -- is not linearizable because of the async nature
          - leaderless -- almost never linearizable
               - even when strict quorum (w + r > n), due to network latency, when 2 reads concurrent when a write, a former read may still see new value when a later read sees old value
               - sloppy quorum violates linearizability, and LWW (last write wins) relies on time-of-day clocks which is nonlinearizable
          - linearizable algos -- the only guarantee
     - the cost of linearizability
          - the CAP theorem
               - when network partitioning happens (some nodes detached from network), either db is available but not linearizable/consistent, or db is linearizable but not all available
               - e.g. think of multi leader db in separate datacenters, available but 2 dc not consistent
               - e.g. for single leader db, when network is partitioned, the partition without the leader can't accept writes
          - linearizable systems' response time at least proportional to network uncertainly, so performance is a big issue
          - most dbs don't have linearizability, some has weaker consistency models
- ordering guarantees
     - causality consistent: a system that obeys the rule of causality
     - weaker than linearizability, but doesnt' have the coordination overhead, and less sensitive to network problems
     - ordering and causality
     - causal order is not a total order
          - total order: everything can be ordered; partial order: some items can't be ordered
          - a linearizable system: total order because there has to be a point where things happen atomically
          - a causality consistent system: partially ordered because there are concurrent requests
     - linearizability is stronger than causal consistency
          - linearizable system preserves causal consistency, but performance takes a hit
          - causal consistency is the strongest consistency level that doesn't hurt performance when network is bad
     - implementation for causal consistency
          - to preserve causality is to know which happens before which, and make sure it happens in all replicas
          - to know the partial ordering, similar to detecting concurrent writes, not only to single objects, but across the entire table
          - can use generalized version vectors
     - sequence number ordering
          - assign sequence numbers (a.k.a logical timestamps) to operations to create a total ordering (concurrent ones assigned arbitrarily) --> consistent with causality, especially single leader so that follower follows leader causally
          - if not single leader (either partitioned or multi/leaderless), ways to generate sequence numbers:
               - each node generates its own, preserve some bits to indicate node numbers
               - attach number with time-of-day clock if resolution is accurate enough
               - preallocate blocks of range for different nodes
               -> scalable, but none of them preserves causality in their ordering
          - can use the Lamport timestamps to do a total ordering of events on distributed system
               - Lamport timestamps are pairs (counter, nodeID) -- compare counter, if counter are same, compare nodeID
               - for each request, node increment the counter id by 1 over the id sent by client
               - for each request, client keeps the max counter id replied from nodes, and include that in the next request
               - by client sending requests to different nodes, its id essentially syncs the order happens around all nodes (max of all nodes)
               - concurrent operations on different nodes may have the same counter but different nodeID
          - timestamp ordering (like Lamport) is not sufficient
               - because the total ordering is finalized after all ids are collected
               - for unique constraints problem, two concurrent operations can't know what the other nodes are doing at that point (if do so synchronously then if some node goes down the system fails)
               - need to use broadcasting to know when the ordering is finalized
        - total order broadcast
           - a protocol for nodes to exchange messages
           - 2 properties always satisfied
                 - reliably delivered to all nodes
                 - totally ordered delivery (all nodes receives msg in the same order)
            - if node faulty, no msg delivered but when it's back, deliveries gets retried and in the same order
            - uses of total order broadcast
                 - db replication (msg as writes to db, all nodes gets them all and gets them ordered) -- state machine replication
                 - like creating a log -- each msg sent is to append to the log, so log is ensured to be the same order on every node
                 - lock service for fencing tokens -- the order that the service arrives are kept in sequence ids, which can be tokens
            - implementing linearizable storage using total order broadcast
            - Implementing total order broadcast using linearizable storage
- distributed transactions and consensus
          - problem: get nodes to agree on something -- actually pretty hard
          - used in situations like leader selection (and reselection in failover) and distributed atomic commit
          - single node commit
               - write data (write ahead log), then write the commit record
               - when crash before commit record, recover from WAL; if crash after commit record, it's considered done
          - challenges for mutli node transactions
               - if ask to commit, individual partitions/obj can fail, leaving partially committed state
               - fail reasons: partition detects constraint violation/conflict, commit msg lost in network, nodes may crash before commit record written
               - no way to recover once commits, because data is available to subsequent transactions
          - 2 Phase Commit
               - the most common distributed transactions implementation
               - there's a transaction coordinator
               - before the transaction, ask coordinator for globally unique transaction ID
               - first each partition to read/write data as normal (abort can happen) -- start holding locks
               - then coordinator send prepare msg to each node (if requests time out or fails, send abort to all nodes)
               - nodes write to disk and check for conflicts etc, then answer "yes" or "no"
               - then coordinator makes a decision, write the commit record (the commit point)
               - coordinator sends "commit" (or abort) requests to each node (if request time out, keep trying until get an answer) -- nodes release locks after commit
          - coordinator failure
               - if coordinator fails after sending out prepare requests, participants can't abort nor commit individually
               - can only wait for coordinator to answer -- when coordinator comes back, check its own transaction log, and if there's no commit record, abort
          - there's a 3 phase commit for nonblocking distributed commits, but it assumes bounded time so unrealistic
     - distributed transactions in practice
          - most dbs don't choose to implement distributed transactions for performance and other troubles except for SQL-series
          - 2 types of distributed transactions
               - db-internal: a db that offers replication&partition natively -- works ok
               - heterogenous: 2+ different technologies, e.g. atomic commit for db + message broker
          - exactly-once msg processing
               - commit the msg acknowledgement and db write (and other side effects) in a single atomic trasaction
               - if either one failed (aborted), the entire transaction gets aborted, and retried
               - requires each component/side effect to support the same atomic commit protocol
          - XA transactions
               - an API implemented by the transaction coordinator and the db/message components
               - calls for components to know if they're part of a distributed transaction, and callbacks for prepare, commit, abort
               - if coordinator process or the server it's on crashes, then server must be restarted to see the transaction log on disk
          - issue for distributed 2pc transactions
               - because components/db hold lock while in doubt, it takes long when coordinator crashes -- sometimes forever the coordinator's log is lost
               - meanwhile other transactions gets blocked -- can't write those locked records (for some db can't even read)
               - in practice, orphaned in-doubt transactions occur because of lost logs/software bugs, and even restarting db node doesn't resolve the problem because it still needs to hold the lock, requiring human to decide under high pressure
               - emergency escape: let node decide unilaterally, but it breaks the atomicity promise
          - limitations for distributed transactions
               - the coordinator is as important as the db itself, but it's usually the single point failure
               - a coordinator deployment changes the nature of stateless applications because it holds states
               - XA doesn't support SSI (detecting conflicts across components) or deadlock detection (passing lock info)
               - if any component fails, transaction fails, so it amplifies failures, defeating the purpose of fault tolerance systems
     - fault tolerant consensus
          - formal definition: some nodes proposes values, then the quorum decides
          - properties of consensus algo: uniform agreement, integrity (can't change mind), validity (must be a value proposed), termination (fault tolerance)
          - algos assume fault-stop model, and more than half of nodes are functional, no byzantine faults
          - total order broadcast is like multiple rounds of consensus: deciding on a total ordering of the next messages
          - the consensus algorithm
               - when a leader seems dead, a new vote happen with a monotonically increasing number (the epoch number)
               - the leader that has the higher epoch number prevails
               - then the leader makes a decision
               - 2 rounds of voting: first to decide on a leader, then to vote on a leader's proposal
               - great because it's fault tolerant while providing safety properties
          - limitations of concensus
               - requires a strictly majority to operate, so the minority partition won't be available
               - requires a fix number of nodes to operate and a small number of nodes to tolerate
               - relies on timeout, so if network unreliable, can stuck in leader voting longer than actually doing any job
     - membership and coordination services
          - zookeeper's feature set
               - linearizable atomic operations: the consensus algo guarantees linearizability, implemented as a lease to be available
               - total ordering of operations: can implement fencing token (for resource protection)
               - failure detection: by heartbeats with client. when session times out, locks are automatically released
               - change notifications: clients can read locks/vals created by other clients and subscribe to them
          - types of works the coordination service can do
               - leader choosing
               - detect if nodes are dead and rebalancing partitioned db
               - zookeeper store info that's slow changing (once every minute/hour)
          - service discovery
               - which ip needs to connect to in order to reach a particular service
               - does not need consensus, but can ask who the leader is and that requires consensus
          - membership services
               - deciding which nodes are alive and which are dead
               - failure detection + consensus to decide if a node is dead -- more reliable, but incorrect declarations can still happen

Chapter 10  Batch Processing
- 3 types of systems
     - services/online systems -- response time and availability are important
     - batch processing systems (offline) -- job often scheduled, measured by throughput
     - stream processing systems (near real time) -- operates on events shortly after they happen, lower latency than batch
- Unix tools
     - log analysis: sorting vs in-memory aggregation
          - in-memory aggregation (hash table) depends on how many keys we have (working set size)
          - sorting can be implemented with in-memory sorting persisted to disk then merge (mergesort, and sequential access too), highly parallelizable and can handle large amounts of data
     - unix design
          - agile and rapid iterations, do one thing and do it well, connect programs using pipes
          - common interface (file descriptors, ascii and parsing by newline characters) allows programs to interoperate well
          - separation of logic and wiring: wiring happens through stdin&stdout, but that limits multiple inputs/network outputs etc
          - encourages experimentation: input file usually immutable (therefore safe), can write to a file to resume a pipeline later
- mapreduce and distributed file systems
     - HDFS (hadoop distributed file system)
          - mapreduced jobs' input and output are files stored on a distributed file system like HDFS
          - shared-nothing across nodes, each nodes has a daemon running to expose file access, and have a central NameNode to keep track of mappings of blocks and data
          - data either replicated (like RAID) or have erasure encoding to recover lost data
     - mapreduce execution
          - steps
               - break file into records (key value pairs), like the \n in unix program
               - call a mapper function to emit (key, value) pairs (0 or 1 or multiple per record)
               - (implicit) sort by key
               - call reducer function to reduce pairs with the same key with a single value
          - distributed execution
               - map is distributed: try to run each task on the server where the file is located to prevent moving bits around; code is copied to servers
               - num of reducers can be configured: usually mappers keep sorted partitions (can be larger than memory) by key and reducers fetch from each mapper machine
               - final output usually written as files, each file per reducer with replication
          - workflows
               - very common to chain multiple mapreduce jobs, e.g. most popular movies, thus workflows
               - workflows are independent jobs that pipes through data written to disk -- must finish before dependents execute
               - can have workflow schedulers to help out with scheduling
     - reduce-side joins and grouping
          - joins in batch processing
               - usually do a full scan of tables, but reasonable because need to resolve all references for all records
               - example: user activity clickstream with user info -- join one by one is limited by network roundtrips and cache depends on data distribution, better to put user table into the same distributed file system as clickstream
               - sort-merge join
                    - two tables gets mapper that maps the same keys, in reducers the keys can be sorted into the same local machine
                    - can have secondary sort that dictates keys from what job gets seen first, and orders within each key
                    - separation of application logic with communication by bringing data into the same place (also auto retry for failures)
          - group by
               - easily implemented by mapreduce, e.g. sessionization (because sessions are stored on different server's logs)
               - handling skew
                    - pig: do a sampling job first, then when actually doing it, distribute hot keys to multiple servers, and the other data input gets replicated into each of these servers
                    - sharded join: need explicitly specify the hot keys
                    - can also do grouping into 2 stages: compaction and a more general aggregation
     - map-side joins
          - pros and cons of reduce side joins
               - bad: put all the workload to reducer, which can be written to disk multiple times -- expensive
               - good: makes no assumptions about input data because it's been prepared by the mapper
          - broadcast hash joins
               - assume one side of join is small enough to fit into memory
               - the small side gets broadcasted into each mapper, store into a hash table for quick lookup when reading the big side
               - or the small side can be stored as a read-only index on disk, so data doesn't have to fit into memory, but in cache
          - partitioned hash joins/bucketed map joins
               - assume that both sides of data are partitioned in the same way (partitioned using the same hash)
               - e.g. both sides has partitioned by last digit of userid, and do joins by userid
               - same number of partitions, and making sure each partition has all the join data we want
               - can probably assume this when data comes from previous mapreduce jobs
          - map-side merge joins
               - when both sides are sorted using that same join key, mapper can do what the reducer does: read both sequentially
               - usually true when there are previous mapreduce jobs
          - workflow with map-side joins
               - join side affects the output
                    - the output of reduce-side joins are sorted by the join key
                    - the output of map-side joins is partitioned and sorted the same way as the large input
               - important to know metadata (partitions, size, sorted, how partitioned etc) when optimizing joins
     - the output of batch workflows
          - building search indexes (used in Lucene/Solr)
               - indexes are mappings from term to a list of documents
               - can rebuild indexes periodically, but can also do it incrementally (write to segments and later merge)
          - key-value stores
               - a common use is to store recommendations (done by mapreduce) into the db so web service can serve these results
               - problems with mapreduce running inside a database (instead of hadoop environment)
                    - overwhelms the db server, causing problems to the other parts of the web server system
                    - low performance -- limited by the roundtrip network latency even if batch is supported
                    - external system connection adds complexity to partially finished jobs being exposed to external systems
               - solution: build a db inside the batch job
                    - input is immutable, and map reduce jobs write outputs in a directory in the distributed system
          - benefits for immutable input and avoiding side-effects
              - easily roll back wrong code, faster feature development (minimizing irreversibility), enabling automatic retries, same input for multiple jobs, separate of concern (job vs wiring)
    - hadoop vs distributed databases: hadoop like a generalization for distributed db
          - diversity of storage: data doesn't need to be premodeled into db schema, early availability is better than careful planning
          - diversity of processing models: write arbitrary code vs SQL queries (but you can, like Hive)
          - designing for frequent faults
              - hadoop tolerates failure by retrying at individual task granularity, and writes to disk eagerly
              - not because hardware faults happen so open; but because tasks can be preempted out by tasks with higher priority
              - in open source cluster schedulers, preemption is less widely used
- beyond mapreduce
    - materialization of intermediate state
          - mapreduce writes to disk for each task
              - for a series (chained) jobs, a job can start only when previous jobs are completely finished (can with long tail)
              - mappers usually redundant (just read the data in or something)
              - intermediate state also stored with replication across several nodes, overkill
         - dataflow engines
              - more flexible ways to do jobs: operators instead of map reduce iterations, so
              - sorting not performed if not needed, and no unnecessary map jobs
              - data localization optimizations
              - sufficient for intermediate state to be kept in memory or written to local disk
          - fault tolerance
              - not written to disk, so have some way to keeping data transformation ancestry (rdd in Spark and checkpoint operator state in Flink)
              - deterministic is important if recompute data, so can use seed for random number generators
          - an operator that requires sorting needs to be synchronized, but otherwise operators can be piped
    - graphs and iterative processing
          - graph dbs usually has iterative algos -- info propagates along edges, and repeat until some condition is met
          - inefficient for mapreduce because each iteration is a mapreduce job even if only a small part of the graph changes
          - solution: the pregel (bulk synchronous parallel) model: in each iteration, a function is called for each vertex and send msgs
          - fault tolerance: periodically checkpointing (write to disk) state for all vertices, rollback and recompute is things go wrong
          - lots of inter machine communications because no way to optimize for locality, so still not as efficient
    - high level APIs and languages
          - e.g. hive, pig, cascading, crunch -- less code, enables experimentation
          - trend: move towards declarative to better optimize while having ability to maintain flexibility (can use arbitrary code libraries), also less CPU overhead for things like select fields
          - trend: specialization for different domains: nearest neighbors, machine learning etc

Chapter 11  Stream Processing
- transmitting event streams
    - events: immutable chunks of data, encoded, grouped into a topic or stream
    - messaging systems
          - different messaging systems differ on 2 decisions
              - what if producers sends msg faster than consumers? can discard, buffer in a queue (what if full?), or backpressure (throttle producer)
              - what if nodes crash or temp offline? msg lost or not, depend on application needs
          - direct messaging from producers to consumers
              - using UDP, or consumer exposes webhook
              - fault tolerance is low and application needs to handle retries or lost data
          - message brokers
              - use a broker, and broker needs to handle availability, handles queuing too (may or may not write to disk)
              - difference with db
                   - not long term because msg auto deleted
                   - working set small, and may get overloaded then bad throughput
                   - doesn't support searching/secondary indexes, only primitive features like filtering
                   - notify clients when data changes (unlike db)
          - if multiple consumers
              - load balancing (send to one consumer, round robin), and consumers process msg in between them
              - fan-out, sending to each consumer
              - combination of both: fan-out to both groups, and in each group send to one consumer
          - ack and redelivery
              - if msg processed and ack got lost, msg gets delivered twice
              - if one consumer goes offline and redeliver to another consumer, order of msgs gets messed up
    - log based message brokers
          - append-only log, divided by topics, each topic partitioned for consumers to consume -- broker keep an offset for clients
          - supports fan-out (each consumer keeps separate offset)
          - load balancing by assigning each partition to each consumer (#consumers limited, and later msgs gets hold up if slow)
          - failure tolerance: if a consumer goes down, a new one picks up from its offset (but may lead to 1 duplicate)
          - disk space
              - if disk gets filled up, segments of msg is deleted, but disk is large -- can monitor this and set alert
              - throughput more stable (and fast if from multiple partitions) compared to in-memory brokers writing to disk
          - if consumer can't catch up, only that consumer is affected and others are fine. but in-memory brokers might get memory eaten up by one slow (or shut down) consumer
          - can also replay old msgs by moving the offset
- DB vs streams
    - keeping heterogenous data system in sync is a tricky problem
          - periodical full db dump to another system -- slow
           - dual writes to different systems -- race condition and fault-tolerance (commit) problems
          - solution: change data capture (CPC)
    - change data capture
          - feed data change of one (primary) system into other systems asychrously
          - implementation: db triggers (fragile), and some customized implementations for some dbs
           - initial snapshot needed if not storing every change from the start (need to be consistent)
          - or log compaction to periodically compact those logs -- ideally close to the same size as the db itself after compaction
          - some modern dbs have api to support change streams, and put them into a msg broker
    - event sourcing
          - similar to change data capture except modeled at application level instead of db level
               - also append only, and keeps all events history
               - e.g. translate a command (student 24875 canceled a course enrollment) into an event
               - benefit: when new feature are introduced that side effect can be chained off some events
          - deriving current state from event log
               - usually needs reconstruction of current state, either by snapshotting or replay
               - but log compaction is not possible because usually only part of the state changes, instead of cdc's full overwrite
           - commands and events
               - command is an action, need to verify constraints before emitting an immutable event
               - or 2 steps, event for temporary action, then async constraint checking, then another event for confirmed action
    - state, streams, immutability
          - state is the integral of the changelogs, and a stream event is a derivative of the state
          - advantages of immutable events
               - easy to diagnose what happened in the system (in terms of bad code recovery)
               - immutable events capture more than current state, useful for data analytics purposes
          - deriving several views from the same event log
               - easy to evolve application: derive a new view and build new feature on that view and run things side by side
               - separation of concerns: view optimized for write gets translated to view optimized for reads, unlike predefined schema
          - concurrency control
               - problem: user writes to the event log and read of log-derived view with a delay
               - solution: perform update to read view synchronously with append log
               - also, log defines a deterministic order of events that needs to be executed, so can do actual serial execution
          - limitations of immutability
               - when state changes too frequently, logs become too large and performance is an issue
               - administrative reasons: legislation for deleting personal data, sensitive data and privacy concerns
               - truly deleting is hard because data lives in multiple places, so only makes it hard to find that data
- processing streams
    - 3 options: write to db/index/storage, push to user (email/notification/dashboard), process in pipeline
    - different: no sorts, fault-tolerance can't be restarting to process from the start
    - stream processing applications
          - complex event processing (CEP): store queries, then watch for data to match up the pattern, and emit complex event
          - stream analytics: usually windows, may use probabilistic algo to approximate
           - maintaining materialized views (keep derived systems up to date): maintain states from the start
          - search on streams: again store queries (might be able to index queries) and do things like full text search
           - message passing in RPC like (actor frameworks) frameworks
    - about time
          - event time vs processing time: could lead to bad orders and sudden spikes after failover
          - due to delays msg from previous windows might come way later after that window is published -- drop or do a correction
          - device clock and server clock doesn't correspond
               - device clock is intentionally wrong, or network not available for several hours or days
               - solution: record 1)device clocks event occur time, 2)device clock's send time, 3)server's received time, estimate real occur time with 3 - 2 + 1 (omitting network delay)
          - types of windows
               tumbling window: fixed length, e.g. event timestamp rounded down to whole minutes
               hopping window: fixed length but overlapping, e.g. rounded down and sum nearby 5 minutes, each time moving 1 minute
               sliding window: keep a buffer of events and kick them out when expire
               session window: no fixed duration but group events by session id
    - stream joins
          - stream-stream join (window join)
               - e.g. stream 1 is events for search results, stream 2 is events for click results
               - implementation: maintain a state for the past hour, a window featuring both streams indexed by session id, if there's an item matching stream 1 and 2, emit "click", if expires, emit "nonclick"
          - stream-table join (stream enrichment) (one is table change logs)
               - e.g. join stream of user actions with a user info table, slow if making request to table every time
               - implementation: stream processor keep local copy in memory, and subscribe to change data capture of the user table
          - table-table join (materialized view maintenance) (both streams are table change logs)
               - e.g. push tweets to followers, and push recent tweets to new follower
               - equivalent to join the follower table with tweets table, like maintaining a cache
               - implementation: stream processor maintains mapping of (user, followers), updated when the underlying table changes
          - time-dependence of joins
               - problem: there could be delays in processing, but at processing time looking at a changed table to join
               - solution: give an id for each version of the joined record (usually the table), to make sure it's determinist
               - downside: can't use log compaction because need to remember all versions
    - fault tolerance
          - microbatching and checkpointing
               - microbatching: keep a tumbling window (like 1 sec), rerun window if failure
               - checkpointing: periodically write checkpoint to disk, discard last checkpoint til crash if failure
               - problem: output to external places (the side effect) happens twice if failure happens after things were sent
          - atomic commit
               - use distributed atomic commit for 2 internal systems: processing and side effect to ensure "exact once processing"
          - idempotence
               - idempotence: thing you can do multiple times with the same effect as if just once
               - can be done by keeping external metadata, like monotonically increasing id (offset) to tell if thing are done already
               - during failover, use fencing to prevent a node that's thought to be dead but is alive
          - rebuilding state after failure
               - a requirement to recover after a failure
               - replay if window is short
               - keep remote state, or keep state (or snapshot) local and replicate periodically

Chapter 12 The Future of Data Systems
         
